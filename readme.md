# Agentic AI Approach for Online Fraud Detection

This repository contains an **Agentic AI** pipeline for **online credit card fraud detection**. It leverages multiple specialized agents, semantic retrieval, LLM-based reasoning, a human-in-the-loop (HITL) process, and comprehensive evaluation metricsâ€”all containerized via Docker Compose.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ retriever_agent.py       # Crafts semantic queries to FAISS
â”‚   â”œâ”€â”€ fraud_analyst.py         # Main fraud analysis logic
â”‚   â””â”€â”€ report_generator.py      # Generates alert & audit-ready report
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ retrieval_task.py        # Defines retrieval task & prompt
â”‚   â”œâ”€â”€ analysis_task.py         # Analyzes transactions for fraud
â”‚   â””â”€â”€ report_task.py           # Generates alert and fraud report
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ vector_search_tool.py    # FAISS-based search tool
â”œâ”€â”€ data/
â”‚   â””â”€â”€ customer_transaction_history.csv
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ all_results.json         # Model predictions with latency
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluator.py             # Evaluation metrics & visualization
â”‚   â”œâ”€â”€ ground_truth1.json       # Ground truth labels
â”‚   â”œâ”€â”€ evaluation_metrics.json  # Computed metrics (precision, recall, F1, AUC-PR, latency)
â”‚   â”œâ”€â”€ evaluation_metrics.md    # Markdown report
â”‚   â”œâ”€â”€ evaluation_confusion_matrix.png
â”‚   â””â”€â”€ evaluation_aucpr.png
â”œâ”€â”€ transaction/
â”‚   â””â”€â”€ sample.py                # Sample transaction for testing
â”œâ”€â”€ main.py                      # Orchestrates CrewAI pipeline
â”œâ”€â”€ Dockerfile                   # Container setup
â”œâ”€â”€ docker-compose.yml           # Services + interactive HITL
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ environment.yml              # Conda environment containing all dependencies
â”œâ”€â”€ .env                         # Environment variables
â””â”€â”€ README.md                    # This file
```

---

## ğŸ› ï¸ Technology Stack

- **CrewAI** for multi-agent orchestration
- **LangChain** + **FAISS** for vector retrieval
- **OpenAI API** for LLM reasoning
- **Python 3.10+**, Pydantic for schemas
- **Scikit-learn, NumPy** for evaluation metrics
- **Matplotlib, Seaborn** for visualization
- **Docker Compose** for containerized, interactive deployment

---

## ğŸ“¦ Dependencies

The project requires the following key dependencies (see `requirements.txt` for the full list):

- Python 3.10+
- CrewAI
- LangChain and langchain-community components
- FAISS for vector search
- OpenAI API client (`langchain_openai`)
- Pydantic (v2)
- NumPy, Scikit-learn (for evaluation)
- Matplotlib, Seaborn (for visualization)
- Docker and Docker Compose (for containerized deployment)

To install locally:

```bash
pip install -r requirements.txt
```

Or using conda (if you have environment.yml):

```bash
conda env create -f environment.yml
conda activate fraud-det
```

---

## ğŸš€ Quick Start

1. **Set your OpenAI API key**\
   Edit the existing `.env` file (already in the project) and add:

   ```bash
   OPENAI_API_KEY=your_real_key_here
   ```

2. **Run without Docker (simple method)**\
   Install dependencies and run the app directly:

   ```bash
   pip install -r requirements.txt
   python main.py
   ```

   This will run locally, using your environment variables from `.env`.

3. **Run with Docker (recommended interactive mode)**\
   For human-in-the-loop (HITL) review and interactive feedback:

   ```bash
   docker-compose run --rm --service-ports -it fraud_detection_ai python main.py
   ```

   - This runs the `fraud_detection_ai` container interactively, with ports exposed.
   - Press **Enter** to approve or type feedback when prompted.

4. **Alternative: Run as a service (non-interactive)**\
   If you prefer to run the entire stack as defined in `docker-compose.yml` without interactive input:

   ```bash
   docker-compose up --build
   ```

   - This uses the default command from the compose file and launches all services.
   - Useful for automated or non-interactive runs.

5. **Run without human input (auto-approve mode)**\
   Edit `tasks/analysis_task.py` and set `human_input=False`, then run with either method above.

6. **View Results**

   - Customer alert appears in console (a brief notification message).
   - Both the short user-facing alert and the detailed fraud team report are currently saved together in `reports/fraud_report.md` and accumulated in `reports/all_results.md`.
   - Model predictions (with latency) are saved to `reports/all_results.json`.

---

## ğŸ“Š Evaluation & Metrics

The repository includes a comprehensive evaluation pipeline (`evaluation/evaluator.py`) to compute and visualize model performance metrics.

### Usage

**Evaluate with manually using your TN, FP, FN and TP** (computes latency from `all_results.json` if present)
```bash
python evaluation/evaluator.py --manual-conf 32 1 3 64 --results reports/all_results.json --ground-truth evaluation/ground_truth1.json
```

- Arguments: `--manual-conf TN FP FN TP` (True Negatives, False Positives, False Negatives, True Positives)
- When using `--manual-conf`, the evaluator computes metrics from confusion counts and optionally extracts AUC-PR and latency statistics from the results file.

### Outputs

The evaluator generates:
- **evaluation_metrics.json** â€“ Precision, Recall, F1-Score, Confusion Matrix, AUC-PR (single point), Latency Stats
- **plot_pr_curve.py** - Precisionâ€“Recall Curve with AUCPR = 0.9889
- **evaluation_metrics.md** â€“ Human-readable markdown report
- **evaluation_confusion_matrix.png** â€“ Heatmap visualization of confusion matrix
- **evaluation_aucpr.png** â€“ Precision-Recall curve (full curve if scores available, single point otherwise)

### Example Output

```json
{
  "precision": 0.9846,
  "recall": 0.9552,
  "f1_score": 0.9697,
  "confusion_matrix": [[32, 1], [3, 64]],
  "counts": {
    "total_evaluated": 100,
    "total_positive": 67,
    "total_predicted_positive": 65
  },
  "aucpr": 0.9889,
  "latency": {
    "mean": 50.40,
    "median": 48.84,
    "min": 30.81,
    "max": 86.74,
    "std": 11.72
  }
}
```

### Latency Field Support

The evaluator extracts latency from result objects using these field names (in order of precedence):
- `latency_seconds` â€“ latency in seconds
- `latency_ms` â€“ latency in milliseconds (auto-converted to seconds)
- `latency` â€“ generic latency field

Ensure your `all_results.json` includes per-request latency for accurate latency statistics.

---

## ğŸ—ï¸ Architecture

1. **Retriever Agent**

   - Crafts 2â€“4 semantic queries as plain strings.
   - Uses `TransactionVectorSearch` (FAISS) to fetch topâ€‘5 similar transactions.

2. **Fraud Analyst Agent**

   - Analyzes a single **target** transaction.
   - Uses retrieved context + heuristic if data is sparse.
   - Outputs risk score, classification, evidence, recommendation.

3. **Human-in-the-Loop**

   - Optional approval or feedback step before final reporting.

4. **Report Generator Agent**

   - Produces:
     - **Customer Notification** (short, plainâ€‘language).
     - **Fraud Team Report** (`fraud_report.md`, auditâ€‘ready).

5. **Evaluation Pipeline**

   - Computes precision, recall, F1, AUC-PR, confusion matrix.
   - Extracts and analyzes latency metrics from model outputs.
   - Generates visualizations (PR curve, confusion matrix heatmap).

---

## âš™ï¸ Configuration

```python
CSV_PATH = "data/..."
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
```

- **Environment Variables**
  - `OPENAI_API_KEY` (required)
  - `HITL_MODE` (optional) â€“ if set to `false`, skips human_input.

---

## ğŸ§ª Testing Sample Transactions

The application includes a predefined `sample_transaction` (in `transaction/sample.py`) that is automatically analyzed on each run, so you can immediately test the system. Currently, the transaction tested is:

```python
sample_transaction = {
    "transaction_id": "b40c083614de1a1c8c8835d4bb01b380",
    "amount": 60.00,
    "category": "gas_transport",
    "location": (35.5494, -80.4226),
    "merchant": "Raynor Feest and Miller",
    "timestamp": "2025-04-15T15:47:12Z",
    "user_id": "4065133387262473",
    "merch_lat": 35.219240,
    "merch_long": -80.402563,
    "trans_time": "15:47:12"
}
```

This transaction is processed by the Retriever Agent (for context retrieval), the Fraud Analyst Agent (for risk scoring and classification), and the Report Generator Agent (for alerts and final reports). The app retrieves any historical context available, analyzes this sample, and produces both a console alert and a detailed markdown report.

To run the test with this sample transaction:

```bash
python main.py
```

or using Docker (interactive HITL):

```bash
docker-compose run --rm --service-ports -it fraud_detection_ai python main.py
```

A brief alert will be shown in the console, and both the alert and detailed fraud report will be saved to `reports/fraud_report.md`.

---

## ğŸ“„ Sample Output

Both the short user-facing alert and the detailed fraud team report are currently saved together in `reports/fraud_report.md`, so you can find both outputs in one file.

**Sample Alert:**

   ```text
   ALERT: Suspicious transaction detected for User 4192832764832. Risk Score: 85 (HIGH). Transaction blocked pending review.
   ```

**Sample Fraud Report:**

```markdown
# Fraud Report

**Transaction ID:** f82dfd045c91110964bcedd1dc0df84e  
**Risk Score:** 85 (HIGH)  
**Key Evidence:**
- Amount (320.99) â‰« user average (45.60)
- Merchant location â‰  home city
**Recommendation:** Block & Escalate  
**Reasoning:** Rapid location deviation and high amount.
```

**Sample Evaluation Metrics (from evaluator.py):**

See `evaluation/evaluation_metrics.md` for formatted metrics report, and `evaluation/evaluation_metrics.json` for raw metrics data.

---

## ğŸ”„ Workflow

1. **Prepare data** â€“ Ensure `data/customer_transaction_history.csv` is populated.
2. **Run fraud detection** â€“ Execute `python main.py` (or via Docker).
3. **Review results** â€“ Check console output and `reports/fraud_report.md`.
4. **Evaluate model** â€“ Run `python evaluation/evaluator.py` with your results.
5. **Analyze metrics** â€“ Review `evaluation/evaluation_metrics.json` and visualizations.

---

## ğŸ“ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/...`)
3. Commit changes (`git commit -m "feat: ..."`)
4. Push (`git push origin feature/...`)
5. Open a Pull Request

---

## ğŸ“š References

1. Lewis *et al.* (2020). *Retrieval-Augmented Generation for Knowledgeâ€‘Intensive NLP Tasks*. [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)
2. Carcillo, Le Borgne, Caelen & Kessaci (2019). *Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection*. [Wikipedia Data Analysis for Fraud Detection](https://en.wikipedia.org/wiki/Data_analysis_for_fraud_detection)
3. Akca *et al.* (2023). *A Systematic Review of Intelligent Systems and Analytic Applications in Credit Card Fraud Detection*. [MDPI Applied Sciences](https://www.mdpi.com/2076-3417/15/3/1356)
4. Bonkoungou, Roy & Ako (2024). *Credit Card Fraud Detection Using ML Techniques*. [SpringerLink](https://link.springer.com/chapter/10.1007/978-981-99-9811-1_2)
5. Business Insider (2025). *At Mastercard, AI is helping to power fraudâ€‘detection systems*. [Business Insider](https://www.businessinsider.com/mastercard-ai-credit-card-fraud-detection-protects-consumers-2025-5)

---

Welcome any feedback!

